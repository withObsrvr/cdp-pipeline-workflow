# Contract Data Schema Mapping in CDP Pipeline

This document explains how the CDP Pipeline handles schema mapping for contract data, from processing through storage in PostgreSQL.

## Overview

The CDP Pipeline uses a configuration-driven approach for defining database schemas. Instead of hardcoding table structures, schemas are defined in YAML configuration files, allowing flexible data extraction and storage without code modifications.

## Architecture

### Data Flow

```
┌─────────────────────┐     ┌──────────────────────┐     ┌─────────────────────────────┐
│BufferedStorageSource│ --> │ContractDataProcessor │ --> │SaveContractDataToPostgreSQL│
└─────────────────────┘     └──────────────────────┘     └─────────────────────────────┘
        Raw Data                  Pass-through              Schema Application & Storage
```

### Key Components

1. **ContractDataProcessor** (`processor_contract_data.go`)
   - Processes raw contract data changes from the blockchain
   - Does NOT apply schemas - passes data through
   - Emits `ContractDataMessage` with raw contract data

2. **SaveContractDataToPostgreSQL** (`consumer/save_contract_data_to_postgresql.go`)
   - Reads schema configuration from pipeline YAML
   - Creates database tables and indexes
   - Extracts fields from JSON data
   - Stores data according to schema

3. **FieldExtractor** (`consumer/field_extractor.go`)
   - Extracts values from nested JSON structures
   - Converts data types for PostgreSQL
   - Handles default values and validation

4. **SchemaManager** (`consumer/schema_manager.go`)
   - Generates SQL DDL statements
   - Creates tables and indexes
   - Validates table/column names

## Schema Configuration

Schemas are defined in the pipeline configuration file under the consumer's `fields` section:

### Example Configuration

```yaml
pipelines:
  ContractDataPipeline:
    consumers:
      - type: SaveContractDataToPostgreSQL
        config:
          table_name: "contract_data"
          create_table_if_not_exists: true
          
          fields:
            # Generated fields (handled by PostgreSQL)
            - name: "id"
              type: "SERIAL PRIMARY KEY"
              generated: true
              
            - name: "created_at"
              type: "TIMESTAMP DEFAULT NOW()"
              generated: true
              
            # Data extraction fields
            - name: "contract_id"
              source_path: "contract_id"
              type: "TEXT"
              required: true
              
            - name: "key_decoded"
              source_path: "contract_data.key_decoded"
              type: "JSONB"
              required: false
              
            - name: "ledger_sequence"
              source_path: "ledger_sequence"
              type: "BIGINT"
              required: true
              
          indexes:
            - name: "idx_contract_ledger"
              columns: ["contract_id", "ledger_sequence"]
              type: "btree"
              
            - name: "idx_key_decoded_gin"
              columns: ["key_decoded"]
              type: "gin"
```

### Field Configuration Options

Each field can have the following properties:

- **name**: Database column name
- **source_path**: JSON path to extract data from (uses gjson syntax)
- **type**: PostgreSQL data type (TEXT, BIGINT, JSONB, BOOLEAN, TIMESTAMP, etc.)
- **required**: Whether the field must exist in the source data
- **generated**: Whether the field is auto-generated by PostgreSQL
- **default**: Default value if field is missing

### Source Path Syntax

The `source_path` uses [gjson](https://github.com/tidwall/gjson) syntax for navigating JSON:

- Simple field: `"contract_id"`
- Nested field: `"contract_data.key_decoded"`
- Array access: `"items.0.value"`
- Deep nesting: `"contract_data.val_decoded.data.amount"`

## How It Works

### 1. Table Creation

When the consumer starts, if `create_table_if_not_exists` is true:

1. SchemaManager reads the field definitions
2. Generates a CREATE TABLE statement with all columns
3. Executes the statement to create the table
4. Creates all defined indexes

Example generated SQL:
```sql
CREATE TABLE IF NOT EXISTS contract_data (
    id SERIAL PRIMARY KEY,
    created_at TIMESTAMP DEFAULT NOW(),
    contract_id TEXT NOT NULL,
    key_decoded JSONB,
    ledger_sequence BIGINT NOT NULL
);

CREATE INDEX idx_contract_ledger ON contract_data (contract_id, ledger_sequence);
CREATE INDEX idx_key_decoded_gin ON contract_data USING gin (key_decoded);
```

### 2. Data Processing

For each incoming message:

1. FieldExtractor iterates through configured fields
2. For each non-generated field:
   - Extracts value using `source_path`
   - Converts to specified PostgreSQL type
   - Applies validation (required fields)
   - Uses default value if configured
3. Builds INSERT statement with extracted values
4. Executes the insert

### 3. Type Conversion

The FieldExtractor automatically converts between JSON and PostgreSQL types:

| JSON Type | PostgreSQL Types | Notes |
|-----------|------------------|-------|
| String | TEXT, VARCHAR | Direct mapping |
| Number | INTEGER, BIGINT, NUMERIC | Converts based on size |
| Boolean | BOOLEAN | true/false mapping |
| Object/Array | JSON, JSONB | Stores as JSON |
| String (ISO8601) | TIMESTAMP | Parses datetime strings |

## Supported Index Types

The system supports all major PostgreSQL index types:

- **btree**: Default, good for equality and range queries
- **hash**: Fast equality comparisons
- **gin**: Full-text search and JSONB queries
- **gist**: Geometric and full-text search
- **spgist**: Space-partitioned GiST
- **brin**: Block range indexes for large tables

## Practical Example

Given this contract data JSON:
```json
{
  "contract_id": "CASJKXVOKEBFC6Z3JLW2NNVGMKLDGXR3O5NPVGZY4FIUXTNMQPBQYLZI",
  "ledger_sequence": 426050,
  "contract_data": {
    "contract_key_type": "INSTANCE",
    "key_decoded": {
      "type": "Symbol",
      "value": "Admin"
    },
    "val_decoded": {
      "type": "Address",
      "value": "GABC123..."
    },
    "last_modified_ledger": 426050
  },
  "timestamp": "2024-01-15T10:30:00Z"
}
```

With the configuration above, the consumer would:

1. Extract `contract_id` → "CASJKXVO..."
2. Extract `ledger_sequence` → 426050
3. Extract `contract_data.key_decoded` → {"type": "Symbol", "value": "Admin"}
4. Auto-generate `id` and `created_at`
5. Insert the row into PostgreSQL

## Best Practices

1. **Use JSONB for flexible data**: Store complex or variable structures as JSONB
2. **Create appropriate indexes**: Use GIN indexes for JSONB queries
3. **Mark required fields**: Ensure data integrity with `required: true`
4. **Use generated timestamps**: Let PostgreSQL handle `created_at` fields
5. **Validate configurations**: Test with small data ranges first
6. **Consider performance**: Balance between normalized fields and JSONB storage

## Differences from Contract Invocation Extraction

It's important to note that contract data processing is separate from contract invocation extraction:

- **Contract Data**: Stores raw state changes, schema in consumer config
- **Contract Invocations**: Extracts business logic, uses separate extraction schemas
- Both can run simultaneously on the same data with different schemas

## Configuration File Location

Schema configurations are typically stored in:
- `config/base/contract_data_postgresql_example.secret.yaml`
- Or any custom pipeline configuration file

The `.secret.yaml` extension indicates files containing database credentials that should not be committed to version control.